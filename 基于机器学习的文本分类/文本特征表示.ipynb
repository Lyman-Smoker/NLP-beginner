{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words、TF-IDF、N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Bag of words词袋模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于文本的BoW模型的一个简单例子，首先给出两个简单的文本文档如下：\n",
    "    \n",
    "    文档1：John likes to watch movies. Mary likes too.\n",
    "    文档2：John also likes to watch football games.\n",
    "\n",
    "基于上述两个文档中出现的单词，构建如下一个词典 (dictionary)：\n",
    "\n",
    "    Vocabulary=  {\"John\": 1, \"likes\": 2,\"to\": 3, \"watch\": 4, \"movies\": 5,\"also\": 6, \"football\": 7, \"games\": 8,\"Mary\": 9, \"too\": 10}\n",
    "\n",
    "上面的词典中包含10个单词, 每个单词有唯一的索引, 那么每个文本我们可以使用一个10维的向量来表示。（用整数数字0~n（n为正整数）表示某个单词在文档中出现的次数）：\n",
    "\n",
    "    文档1：[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\n",
    "    文档2：[1, 1,1, 1, 0, 1, 1, 1, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 0,\n",
       " 'likes': 1,\n",
       " 'to': 2,\n",
       " 'watch': 3,\n",
       " 'movies': 4,\n",
       " 'Mary': 5,\n",
       " 'too': 6,\n",
       " 'also': 7,\n",
       " 'football': 8,\n",
       " 'games': 9}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=['John likes to watch movies. Mary likes too.','John also likes to watch football games.']\n",
    "idx=0\n",
    "vocabulary={}\n",
    "# split the sentences to several tokens and construct the vacabulary dictionary \n",
    "for sentence in dataset:\n",
    "    # clear the stop words( here I just clear the dot)\n",
    "    sentence=sentence.replace('.','')\n",
    "    words=sentence.split(' ')\n",
    "    for i in words:\n",
    "        if i not in vocabulary:\n",
    "            vocabulary[i]=idx\n",
    "            idx+=1\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we create a bag of words for each sentence in the dataset \n",
    "encoded_dataset=[]\n",
    "for sentence in dataset:\n",
    "    sentence=sentence.replace('.','')\n",
    "    words=sentence.split(' ')\n",
    "    encoded_vector=[0]*idx\n",
    "    for i in words:\n",
    "        encoded_vector[vocabulary[i]]+=1\n",
    "    encoded_dataset.append(encoded_vector)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、TF-IDF 词频-逆文件频率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词频TF（item frequency）：\n",
    "    某一给定词语在该文本中出现次数。该数字通常会被归一化（分子一般小于分母），以防止它偏向长的文件，因为不管该词语重要与否，它在长文件中出现的次数很可能比在段文件中出现的次数更大。\n",
    "    \n",
    "    TF(word)=k/n,其中n表示文本词数，k表示word出现的词数\n",
    "\n",
    "逆向文件频率IDF（inverse document frequency）：\n",
    "    一个词语普遍重要性的度量。主要思想是：如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n",
    "    \n",
    "    IDF(word)=log(N/N(word)),其中N表示word在语料库中出现的次数，N(word)表示出现word的文本数\n",
    "\n",
    "因此，有TF-IDF计算公式：\n",
    "    \n",
    "    TF-IDF=TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cats', 'have', 'four', 'legs'],\n",
       " ['cats', 'and', 'dogs', 'are', 'antagonistic'],\n",
       " ['he', 'hate', 'dogs']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "corpus = ['Cats have four legs',\n",
    "          'Cats and dogs are antagonistic',\n",
    "          'He hate dogs']\n",
    "# here we want to calculate the TF-IDF of the word \"cat\"\n",
    "\n",
    "# 1、lower each litter and clear the dots\n",
    "lower_corpus=[s.lower().replace('.',' ') for s in corpus]\n",
    "\n",
    "# 2、tokenization\n",
    "tokenized_corpus=[s.split(' ') for s in lower_corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.2, 0.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3、 TF\n",
    "k=[]\n",
    "N_cat=0\n",
    "n=[]\n",
    "for s in tokenized_corpus:\n",
    "    if s.count('cats') != 0:\n",
    "        N_cat+=1\n",
    "        k+=[s.count('cats')]\n",
    "    if s.count('cats') == 0:\n",
    "        k+=[0]\n",
    "    n+=[len(s)]\n",
    "TF=[]\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    TF += [k[i]/n[i]]\n",
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17609125905568124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4、 IDF\n",
    "IDF=math.log((len(tokenized_corpus)/(N_cat)),10)\n",
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04402281476392031, 0.03521825181113625, 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5、TF-IDF\n",
    "TF_IDF=[TF_i*IDF for TF_i in TF]\n",
    "TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary       and  antagonistic       are      cats      dogs      four  \\\n",
      "sentence1   0.000000      0.000000  0.000000  0.402040  0.000000  0.528635   \n",
      "sentence2   0.490479      0.490479  0.490479  0.373022  0.373022  0.000000   \n",
      "sentence3   0.000000      0.000000  0.000000  0.000000  0.473630  0.000000   \n",
      "\n",
      "vocabulary      hate      have        he      legs  \n",
      "sentence1   0.000000  0.528635  0.000000  0.528635  \n",
      "sentence2   0.000000  0.000000  0.000000  0.000000  \n",
      "sentence3   0.622766  0.000000  0.622766  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# also we can use the class 'TfidfVectorizer' of sklearn to calculate the TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    " \n",
    "corpus = ['Cats have four legs',\n",
    "          'Cats and dogs are antagonistic',\n",
    "          'He hate dogs']\n",
    " \n",
    "tfidf = TfidfVectorizer()\n",
    "vect = tfidf.fit_transform(corpus)\n",
    " \n",
    "df = pd.DataFrame()\n",
    "df['vocabulary'] = tfidf.get_feature_names()\n",
    "df['sentence1'] = vect.toarray()[0]\n",
    "df['sentence2'] = vect.toarray()[1]\n",
    "df['sentence3'] = vect.toarray()[2]\n",
    "df.set_index('vocabulary', inplace=True)\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
